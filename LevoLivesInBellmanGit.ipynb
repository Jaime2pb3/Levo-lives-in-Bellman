{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d598d125",
      "metadata": {
        "id": "d598d125"
      },
      "source": [
        "\n",
        "# LevoBellmanOptim‑ZO (no backprop)\n",
        "**Idea:** Hybrid optimization that combines an **analytical** signal (Bellman / TD) with a **reflexive** phase (*levo*, left-handed heuristic) **without using backpropagation**.\n",
        "It updates with **zero order** (ES/SPSA-like) directly in the parameter space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5b2b2ae",
      "metadata": {
        "id": "d5b2b2ae"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Environment and Seed Info ---\n",
        "import os, random, numpy as np, sys\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "try:\n",
        "    import tensorflow as tf, keras\n",
        "    tf.random.set_seed(0)\n",
        "    print(\"TF:\", tf.__version__, \"| Keras:\", keras.__version__)\n",
        "except Exception as e:\n",
        "    print(\"TF/Keras no disponibles:\", e)\n",
        "\n",
        "import torch, sklearn\n",
        "torch.manual_seed(0)\n",
        "\n",
        "print(\"Torch:\", torch.__version__, \"| Sklearn:\", sklearn.__version__)\n",
        "print(\"Python:\", sys.version)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8186bb60",
      "metadata": {
        "id": "8186bb60"
      },
      "source": [
        "## Synthetic data and utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46a02b4e",
      "metadata": {
        "id": "46a02b4e"
      },
      "outputs": [],
      "source": [
        "\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "\n",
        "# Synthetic dataset: states s, actions a, rewards r, next states s'\n",
        "def make_synth_batch(batch_size=64, d_state=5, n_actions=3, gamma=0.97) -> dict:\n",
        "    s  = np.random.rand(batch_size, d_state).astype(\"float32\")\n",
        "    sp = np.random.rand(batch_size, d_state).astype(\"float32\")\n",
        "    a  = np.random.randint(0, n_actions, size=(batch_size,)).astype(\"int64\")\n",
        "    r  = (np.random.randn(batch_size) * 0.1).astype(\"float32\")\n",
        "    done = (np.random.rand(batch_size) < 0.05).astype(\"float32\")\n",
        "    return {\"s\": s, \"sp\": sp, \"a\": a, \"r\": r, \"done\": done, \"gamma\": gamma, \"n_actions\": n_actions}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "715e8cfc",
      "metadata": {
        "id": "715e8cfc"
      },
      "source": [
        "## Keras autoencoder (resonance) and left-handed heuristics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d00068d2",
      "metadata": {
        "id": "d00068d2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "#AE in Keras to measure (1 - cos) resonance on reconstruction\n",
        "AE_INPUT_DIM = 5\n",
        "try:\n",
        "    from tensorflow.keras.models import Model\n",
        "    from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "    inp = Input(shape=(AE_INPUT_DIM,), name=\"ae_inp\")\n",
        "    z   = Dense(3, activation=\"relu\", name=\"enc\")(inp)\n",
        "    out = Dense(AE_INPUT_DIM, activation=\"sigmoid\", name=\"dec\")(z)\n",
        "    ae_keras = Model(inp, out)\n",
        "    ae_keras.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "\n",
        "    # pre-train with simple noise\n",
        "    X_pre = np.random.rand(1024, AE_INPUT_DIM).astype(\"float32\")\n",
        "    ae_keras.fit(X_pre, X_pre, epochs=5, verbose=0)\n",
        "    print(\"AE Keras listo ✅\")\n",
        "except Exception as e:\n",
        "    ae_keras = None\n",
        "    print(\"No se pudo crear AE Keras:\", e)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from typing import Tuple\n",
        "\n",
        "def levo_izquierda(vec: np.ndarray, ae, orig: np.ndarray, e:int=20, d:float=0.05,\n",
        "                   lr:float=0.1, cool:float=0.90, bias:float=-0.35) -> Tuple[np.ndarray, float]:\n",
        "    \"\"\"\n",
        "       Left-handed (push-pull) heuristic on the input vector.\n",
        "       vec: (d,) starting point\n",
        "       ae: model with .predict(vec[None, :]) -> (1, d)\n",
        "       orig: (d,) target vector for measuring resonance (cosine with reconstruction)\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(0)  # reproducible per call\n",
        "    best = vec.copy()\n",
        "    best_cos = -1.0\n",
        "    mod = vec.copy()\n",
        "    delta = float(d)\n",
        "\n",
        "    for _ in range(e):\n",
        "        phase = 2*np.pi*(mod + bias)       # left-handed turn (negative bias)\n",
        "        prop = np.clip(\n",
        "            -mod + rng.uniform(-delta, delta, vec.shape) + 0.1*np.sin(phase),\n",
        "            0, 1\n",
        "        )\n",
        "        rec_prop = ae.predict(prop.reshape(1, -1), verbose=0)\n",
        "        cos_prop = float(cosine_similarity(orig.reshape(1, -1), rec_prop)[0, 0])\n",
        "\n",
        "        rec_mod = ae.predict(mod.reshape(1, -1), verbose=0)\n",
        "        cos_mod = float(cosine_similarity(orig.reshape(1, -1), rec_mod)[0, 0])\n",
        "\n",
        "        if cos_prop >= cos_mod:   # acceptance not to get worse\n",
        "            mod = prop\n",
        "        if cos_prop > best_cos:\n",
        "            best_cos = cos_prop\n",
        "            best = prop.copy()\n",
        "\n",
        "        delta *= max(1e-4, (1 - cos_prop) * lr)\n",
        "        delta = float(np.clip(delta * cool, 1e-5, 0.25))\n",
        "\n",
        "    return best, best_cos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4983df58",
      "metadata": {
        "id": "4983df58"
      },
      "source": [
        "## Q‑network (PyTorch) and Bellman loss**whitout backward**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65187b17",
      "metadata": {
        "id": "65187b17"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch, torch.nn as nn\n",
        "\n",
        "class QNet(nn.Module):\n",
        "    def __init__(self, d_state=5, n_actions=3, hidden=32):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_state, hidden), nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
        "            nn.Linear(hidden, n_actions)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def bellman_loss_forward(qnet: nn.Module, tgt: nn.Module, batch: dict) -> torch.Tensor:\n",
        "    \"\"\" TD error forward-only (no backward) \"\"\"\n",
        "    s  = torch.from_numpy(batch[\"s\"])\n",
        "    sp = torch.from_numpy(batch[\"sp\"])\n",
        "    a  = torch.from_numpy(batch[\"a\"])\n",
        "    r  = torch.from_numpy(batch[\"r\"])\n",
        "    done = torch.from_numpy(batch[\"done\"])\n",
        "    gamma = batch[\"gamma\"]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        q_sp = tgt(sp).max(dim=1).values\n",
        "        target = r + (1.0 - done) * gamma * q_sp\n",
        "\n",
        "    q_sa = qnet(s).gather(1, a.view(-1,1)).squeeze(1)\n",
        "    td = (target - q_sa)\n",
        "    return (td * td).mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d6ad0ce",
      "metadata": {
        "id": "8d6ad0ce"
      },
      "source": [
        "## **LevoBellmanOptim‑ZO** (orden cero; actualización estilo Lion‑sign con weight‑decay desacoplado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "118d4fa1",
      "metadata": {
        "id": "118d4fa1"
      },
      "outputs": [],
      "source": [
        "\n",
        "from typing import Callable, List\n",
        "import torch\n",
        "\n",
        "def collect_params(model: nn.Module) -> List[torch.Tensor]:\n",
        "    return [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "def clone_like(params: List[torch.Tensor]) -> List[torch.Tensor]:\n",
        "    return [torch.zeros_like(p) for p in params]\n",
        "\n",
        "@torch.no_grad()\n",
        "def _snapshot(params: List[torch.Tensor]) -> List[torch.Tensor]:\n",
        "    return [p.detach().clone() for p in params]\n",
        "\n",
        "@torch.no_grad()\n",
        "def _assign(params: List[torch.Tensor], src: List[torch.Tensor]):\n",
        "    for p, s in zip(params, src):\n",
        "        p.copy_(s)\n",
        "\n",
        "@torch.no_grad()\n",
        "def _add_scaled(params: List[torch.Tensor], base: List[torch.Tensor],\n",
        "                eps: List[torch.Tensor], alpha: float):\n",
        "    for p, b, e in zip(params, base, eps):\n",
        "        p.copy_(b + alpha * e)\n",
        "\n",
        "@torch.no_grad()\n",
        "def _sign_update(params: List[torch.Tensor], g_hat: List[torch.Tensor], lr: float, wd: float=1e-2):\n",
        "    for p, g in zip(params, g_hat):\n",
        "        if wd > 0: p.mul_(1 - lr*wd)     # AdamW-like (decoupled)\n",
        "        p.add_(-lr * torch.sign(g))      # Lion-like (sign)\n",
        "\n",
        "def eval_total_loss_forward(qnet: nn.Module, tgt: nn.Module, batch: dict,\n",
        "                            ae, lam: float,\n",
        "                            levo_fn: Callable) -> torch.Tensor:\n",
        "    # Bellman forward\n",
        "    Lb = bellman_loss_forward(qnet, tgt, batch)\n",
        "\n",
        "    # Levo forward (inputs [0,1])\n",
        "    x = batch[\"s\"]\n",
        "    orig = batch[\"sp\"]\n",
        "    cos_list = []\n",
        "    if ae is not None:\n",
        "        for i in range(min(16, x.shape[0])):\n",
        "            vec = x[i]; tar = orig[i]\n",
        "            _, cosv = levo_fn(vec, ae, tar)\n",
        "            cos_list.append(cosv)\n",
        "    Ll = torch.tensor(float(np.mean([1.0 - c for c in cos_list]))) if len(cos_list)>0 else torch.tensor(0.0)\n",
        "    return Lb + lam * Ll\n",
        "\n",
        "def levobellman_zo_step(qnet: nn.Module, tgt: nn.Module, batch: dict,\n",
        "                        ae, lam: float=0.1, sigma: float=0.02, N: int=8, lr: float=1e-2, wd: float=1e-2,\n",
        "                        levo_fn=levo_izquierda):\n",
        "    params = collect_params(qnet)\n",
        "    base = _snapshot(params)\n",
        "    g_hat = clone_like(params)\n",
        "\n",
        "    for _ in range(N):\n",
        "        eps = [torch.randn_like(p) for p in params]\n",
        "\n",
        "        # theta+\n",
        "        _add_scaled(params, base, eps,  sigma)\n",
        "        Lp = eval_total_loss_forward(qnet, tgt, batch, ae, lam, levo_fn)\n",
        "\n",
        "        # theta-\n",
        "        _add_scaled(params, base, eps, -sigma)\n",
        "        Lm = eval_total_loss_forward(qnet, tgt, batch, ae, lam, levo_fn)\n",
        "\n",
        "        coeff = (Lp - Lm) / (2.0 * sigma)\n",
        "        for g, e in zip(g_hat, eps):\n",
        "            g.add_(coeff * e)\n",
        "\n",
        "    # Average\n",
        "    for g in g_hat:\n",
        "        g.div_(float(N))\n",
        "\n",
        "    # restauration and update\n",
        "    _assign(params, base)\n",
        "    _sign_update(params, g_hat, lr=lr, wd=wd)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00bb0094",
      "metadata": {
        "id": "00bb0094"
      },
      "source": [
        "## Demo rápida (10 iteraciones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87e349ce",
      "metadata": {
        "id": "87e349ce"
      },
      "outputs": [],
      "source": [
        "\n",
        "qnet = QNet(d_state=5, n_actions=3)\n",
        "target_q = QNet(d_state=5, n_actions=3)\n",
        "target_q.load_state_dict(qnet.state_dict())\n",
        "\n",
        "history = {\"LB\": [], \"LL\": [], \"Ltot\": []}\n",
        "\n",
        "for t in range(10):\n",
        "    batch = make_synth_batch(batch_size=64, d_state=5, n_actions=3, gamma=0.97)\n",
        "\n",
        "    # Metrics before the step ZO\n",
        "    Lb0 = float(bellman_loss_forward(qnet, target_q, batch))\n",
        "\n",
        "    # Estimation Resonance\n",
        "    x = batch[\"s\"]; orig = batch[\"sp\"]\n",
        "    cos_list = []\n",
        "    if ae_keras is not None:\n",
        "        for i in range(min(8, x.shape[0])):\n",
        "            _, cosv = levo_izquierda(x[i], ae_keras, orig[i])\n",
        "            cos_list.append(cosv)\n",
        "    Ll0 = float(np.mean([1.0 - c for c in cos_list])) if cos_list else 0.0\n",
        "    Ltot0 = Lb0 + 0.1*Ll0\n",
        "\n",
        "    history[\"LB\"].append(Lb0); history[\"LL\"].append(Ll0); history[\"Ltot\"].append(Ltot0)\n",
        "\n",
        "    # Step ZO\n",
        "    levobellman_zo_step(qnet, target_q, batch, ae=ae_keras, lam=0.1, sigma=0.02, N=8, lr=1e-2, wd=1e-2)\n",
        "\n",
        "    # Soft update target\n",
        "    tau = 0.05\n",
        "    with torch.no_grad():\n",
        "        for p_tgt, p in zip(target_q.parameters(), qnet.parameters()):\n",
        "            p_tgt.copy_( (1-tau)*p_tgt + tau*p )\n",
        "\n",
        "print(\"Done. Iterations:\", len(history[\"LB\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67f74ba1",
      "metadata": {
        "id": "67f74ba1"
      },
      "source": [
        "## Graphics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60b39e22",
      "metadata": {
        "id": "60b39e22"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(history[\"LB\"], label=\"Bellman (LB)\")\n",
        "plt.plot(history[\"Ltot\"], label=\"Total (LB + λ·LL)\")\n",
        "plt.legend(); plt.title(\"Evolution of losses (zero order); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c042ec5",
      "metadata": {
        "id": "7c042ec5"
      },
      "source": [
        "\n",
        "### Notes\n",
        "- Backward() is not used: updates are estimated in zeroth order (perturbations).\n",
        "- The levo term is calculated with an EA in Keras (TF/Keras + Torch coexistence), measuring resonance (1 − cos).\n",
        "- The update step is Lion-like (sign-based) with decoupled weight decay (AdamW-like).\n",
        "- You can enable Lookahead/Weight EMA as an additional tempering step if desired.\n",
        "- Useful hyperparameters: sigma (perturbation scale), N (directions), lr, lam (levo weight).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}