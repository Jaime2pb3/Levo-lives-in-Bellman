{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d598d125",
   "metadata": {},
   "source": [
    "\n",
    "# LevoBellmanOptim‑ZO (no backprop)\n",
    "**Idea:** Hybrid optimization that combines an **analytical** signal (Bellman / TD) with a **reflexive** phase (*levo*, left-handed heuristic) **without using backpropagation**.\n",
    "It updates with **zero order** (ES/SPSA-like) directly in the parameter space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b2b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Environment and Seed Info ---\n",
    "import os, random, numpy as np, sys\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf, keras\n",
    "    tf.random.set_seed(0)\n",
    "    print(\"TF:\", tf.__version__, \"| Keras:\", keras.__version__)\n",
    "except Exception as e:\n",
    "    print(\"TF/Keras no disponibles:\", e)\n",
    "\n",
    "import torch, sklearn\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print(\"Torch:\", torch.__version__, \"| Sklearn:\", sklearn.__version__)\n",
    "print(\"Python:\", sys.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8186bb60",
   "metadata": {},
   "source": [
    "## Synthetic data and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a02b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "\n",
    "# Synthetic dataset: states s, actions a, rewards r, next states s'\n",
    "def make_synth_batch(batch_size=64, d_state=5, n_actions=3, gamma=0.97) -> dict:\n",
    "    s  = np.random.rand(batch_size, d_state).astype(\"float32\")\n",
    "    sp = np.random.rand(batch_size, d_state).astype(\"float32\")\n",
    "    a  = np.random.randint(0, n_actions, size=(batch_size,)).astype(\"int64\")\n",
    "    r  = (np.random.randn(batch_size) * 0.1).astype(\"float32\")\n",
    "    done = (np.random.rand(batch_size) < 0.05).astype(\"float32\")\n",
    "    return {\"s\": s, \"sp\": sp, \"a\": a, \"r\": r, \"done\": done, \"gamma\": gamma, \"n_actions\": n_actions}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715e8cfc",
   "metadata": {},
   "source": [
    "## Keras autoencoder (resonance) and left-handed heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00068d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "#AE in Keras to measure (1 - cos) resonance on reconstruction\n",
    "AE_INPUT_DIM = 5\n",
    "try:\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "    inp = Input(shape=(AE_INPUT_DIM,), name=\"ae_inp\")\n",
    "    z   = Dense(3, activation=\"relu\", name=\"enc\")(inp)\n",
    "    out = Dense(AE_INPUT_DIM, activation=\"sigmoid\", name=\"dec\")(z)\n",
    "    ae_keras = Model(inp, out)\n",
    "    ae_keras.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    # pre-train with simple noise\n",
    "    X_pre = np.random.rand(1024, AE_INPUT_DIM).astype(\"float32\")\n",
    "    ae_keras.fit(X_pre, X_pre, epochs=5, verbose=0)\n",
    "    print(\"AE Keras listo ✅\")\n",
    "except Exception as e:\n",
    "    ae_keras = None\n",
    "    print(\"No se pudo crear AE Keras:\", e)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import Tuple\n",
    "\n",
    "def levo_izquierda(vec: np.ndarray, ae, orig: np.ndarray, e:int=20, d:float=0.05,\n",
    "                   lr:float=0.1, cool:float=0.90, bias:float=-0.35) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "       Left-handed (push-pull) heuristic on the input vector.\n",
    "       vec: (d,) starting point\n",
    "       ae: model with .predict(vec[None, :]) -> (1, d)\n",
    "       orig: (d,) target vector for measuring resonance (cosine with reconstruction)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(0)  # reproducible per call\n",
    "    best = vec.copy()\n",
    "    best_cos = -1.0\n",
    "    mod = vec.copy()\n",
    "    delta = float(d)\n",
    "\n",
    "    for _ in range(e):\n",
    "        phase = 2*np.pi*(mod + bias)       # left-handed turn (negative bias)\n",
    "        prop = np.clip(\n",
    "            -mod + rng.uniform(-delta, delta, vec.shape) + 0.1*np.sin(phase),\n",
    "            0, 1\n",
    "        )\n",
    "        rec_prop = ae.predict(prop.reshape(1, -1), verbose=0)\n",
    "        cos_prop = float(cosine_similarity(orig.reshape(1, -1), rec_prop)[0, 0])\n",
    "\n",
    "        rec_mod = ae.predict(mod.reshape(1, -1), verbose=0)\n",
    "        cos_mod = float(cosine_similarity(orig.reshape(1, -1), rec_mod)[0, 0])\n",
    "\n",
    "        if cos_prop >= cos_mod:   # acceptance not to get worse\n",
    "            mod = prop\n",
    "        if cos_prop > best_cos:\n",
    "            best_cos = cos_prop\n",
    "            best = prop.copy()\n",
    "\n",
    "        delta *= max(1e-4, (1 - cos_prop) * lr)\n",
    "        delta = float(np.clip(delta * cool, 1e-5, 0.25))\n",
    "\n",
    "    return best, best_cos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4983df58",
   "metadata": {},
   "source": [
    "## Q‑network (PyTorch) and Bellman loss**whitout backward**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65187b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch, torch.nn as nn\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, d_state=5, n_actions=3, hidden=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_state, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, n_actions)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def bellman_loss_forward(qnet: nn.Module, tgt: nn.Module, batch: dict) -> torch.Tensor:\n",
    "    \"\"\" TD error forward-only (no backward) \"\"\"\n",
    "    s  = torch.from_numpy(batch[\"s\"])\n",
    "    sp = torch.from_numpy(batch[\"sp\"])\n",
    "    a  = torch.from_numpy(batch[\"a\"])\n",
    "    r  = torch.from_numpy(batch[\"r\"])\n",
    "    done = torch.from_numpy(batch[\"done\"])\n",
    "    gamma = batch[\"gamma\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        q_sp = tgt(sp).max(dim=1).values\n",
    "        target = r + (1.0 - done) * gamma * q_sp\n",
    "\n",
    "    q_sa = qnet(s).gather(1, a.view(-1,1)).squeeze(1)\n",
    "    td = (target - q_sa)\n",
    "    return (td * td).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6ad0ce",
   "metadata": {},
   "source": [
    "## **LevoBellmanOptim‑ZO** (orden cero; actualización estilo Lion‑sign con weight‑decay desacoplado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118d4fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Callable, List\n",
    "import torch\n",
    "\n",
    "def collect_params(model: nn.Module) -> List[torch.Tensor]:\n",
    "    return [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "def clone_like(params: List[torch.Tensor]) -> List[torch.Tensor]:\n",
    "    return [torch.zeros_like(p) for p in params]\n",
    "\n",
    "@torch.no_grad()\n",
    "def _snapshot(params: List[torch.Tensor]) -> List[torch.Tensor]:\n",
    "    return [p.detach().clone() for p in params]\n",
    "\n",
    "@torch.no_grad()\n",
    "def _assign(params: List[torch.Tensor], src: List[torch.Tensor]):\n",
    "    for p, s in zip(params, src):\n",
    "        p.copy_(s)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _add_scaled(params: List[torch.Tensor], base: List[torch.Tensor],\n",
    "                eps: List[torch.Tensor], alpha: float):\n",
    "    for p, b, e in zip(params, base, eps):\n",
    "        p.copy_(b + alpha * e)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _sign_update(params: List[torch.Tensor], g_hat: List[torch.Tensor], lr: float, wd: float=1e-2):\n",
    "    for p, g in zip(params, g_hat):\n",
    "        if wd > 0: p.mul_(1 - lr*wd)     # AdamW-like (decoupled)\n",
    "        p.add_(-lr * torch.sign(g))      # Lion-like (sign)\n",
    "\n",
    "def eval_total_loss_forward(qnet: nn.Module, tgt: nn.Module, batch: dict,\n",
    "                            ae, lam: float,\n",
    "                            levo_fn: Callable) -> torch.Tensor:\n",
    "    # Bellman forward\n",
    "    Lb = bellman_loss_forward(qnet, tgt, batch)\n",
    "\n",
    "    # Levo forward (inputs [0,1])\n",
    "    x = batch[\"s\"]\n",
    "    orig = batch[\"sp\"]\n",
    "    cos_list = []\n",
    "    if ae is not None:\n",
    "        for i in range(min(16, x.shape[0])):\n",
    "            vec = x[i]; tar = orig[i]\n",
    "            _, cosv = levo_fn(vec, ae, tar)\n",
    "            cos_list.append(cosv)\n",
    "    Ll = torch.tensor(float(np.mean([1.0 - c for c in cos_list]))) if len(cos_list)>0 else torch.tensor(0.0)\n",
    "    return Lb + lam * Ll\n",
    "\n",
    "def levobellman_zo_step(qnet: nn.Module, tgt: nn.Module, batch: dict,\n",
    "                        ae, lam: float=0.1, sigma: float=0.02, N: int=8, lr: float=1e-2, wd: float=1e-2,\n",
    "                        levo_fn=levo_izquierda):\n",
    "    params = collect_params(qnet)\n",
    "    base = _snapshot(params)\n",
    "    g_hat = clone_like(params)\n",
    "\n",
    "    for _ in range(N):\n",
    "        eps = [torch.randn_like(p) for p in params]\n",
    "\n",
    "        # theta+\n",
    "        _add_scaled(params, base, eps,  sigma)\n",
    "        Lp = eval_total_loss_forward(qnet, tgt, batch, ae, lam, levo_fn)\n",
    "\n",
    "        # theta-\n",
    "        _add_scaled(params, base, eps, -sigma)\n",
    "        Lm = eval_total_loss_forward(qnet, tgt, batch, ae, lam, levo_fn)\n",
    "\n",
    "        coeff = (Lp - Lm) / (2.0 * sigma)\n",
    "        for g, e in zip(g_hat, eps):\n",
    "            g.add_(coeff * e)\n",
    "\n",
    "    # Average\n",
    "    for g in g_hat:\n",
    "        g.div_(float(N))\n",
    "\n",
    "    # restauration and update\n",
    "    _assign(params, base)\n",
    "    _sign_update(params, g_hat, lr=lr, wd=wd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb0094",
   "metadata": {},
   "source": [
    "## Demo rápida (10 iteraciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e349ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qnet = QNet(d_state=5, n_actions=3)\n",
    "target_q = QNet(d_state=5, n_actions=3)\n",
    "target_q.load_state_dict(qnet.state_dict())\n",
    "\n",
    "history = {\"LB\": [], \"LL\": [], \"Ltot\": []}\n",
    "\n",
    "for t in range(10):\n",
    "    batch = make_synth_batch(batch_size=64, d_state=5, n_actions=3, gamma=0.97)\n",
    "\n",
    "    # Metrics before the step ZO\n",
    "    Lb0 = float(bellman_loss_forward(qnet, target_q, batch))\n",
    "\n",
    "    # Estimation Resonance\n",
    "    x = batch[\"s\"]; orig = batch[\"sp\"]\n",
    "    cos_list = []\n",
    "    if ae_keras is not None:\n",
    "        for i in range(min(8, x.shape[0])):\n",
    "            _, cosv = levo_izquierda(x[i], ae_keras, orig[i])\n",
    "            cos_list.append(cosv)\n",
    "    Ll0 = float(np.mean([1.0 - c for c in cos_list])) if cos_list else 0.0\n",
    "    Ltot0 = Lb0 + 0.1*Ll0\n",
    "\n",
    "    history[\"LB\"].append(Lb0); history[\"LL\"].append(Ll0); history[\"Ltot\"].append(Ltot0)\n",
    "\n",
    "    # Step ZO\n",
    "    levobellman_zo_step(qnet, target_q, batch, ae=ae_keras, lam=0.1, sigma=0.02, N=8, lr=1e-2, wd=1e-2)\n",
    "\n",
    "    # Soft update target\n",
    "    tau = 0.05\n",
    "    with torch.no_grad():\n",
    "        for p_tgt, p in zip(target_q.parameters(), qnet.parameters()):\n",
    "            p_tgt.copy_( (1-tau)*p_tgt + tau*p )\n",
    "\n",
    "print(\"Done. Iterations:\", len(history[\"LB\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f74ba1",
   "metadata": {},
   "source": [
    "## Graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b39e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(history[\"LB\"], label=\"Bellman (LB)\")\n",
    "plt.plot(history[\"Ltot\"], label=\"Total (LB + λ·LL)\")\n",
    "plt.legend(); plt.title(\"Evolution of losses (zero order); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c042ec5",
   "metadata": {},
   "source": [
    "\n",
    "### Notes\n",
    "- Backward() is not used: updates are estimated in zeroth order (perturbations).\n",
    "- The levo term is calculated with an EA in Keras (TF/Keras + Torch coexistence), measuring resonance (1 − cos).\n",
    "- The update step is Lion-like (sign-based) with decoupled weight decay (AdamW-like).\n",
    "- You can enable Lookahead/Weight EMA as an additional tempering step if desired.\n",
    "- Useful hyperparameters: sigma (perturbation scale), N (directions), lr, lam (levo weight).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
